{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10212262,"sourceType":"datasetVersion","datasetId":6311917}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U scikit-learn==1.3.2 imbalanced-learn==0.11.0 --quiet\n!pip install --upgrade lightgbm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:35:31.005266Z","iopub.execute_input":"2025-09-20T07:35:31.005601Z","iopub.status.idle":"2025-09-20T07:35:39.061822Z","shell.execute_reply.started":"2025-09-20T07:35:31.005578Z","shell.execute_reply":"2025-09-20T07:35:39.060575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/engine-fault-detection-data/engine_fault_detection_dataset.csv\")  \ndf.head()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-20T05:26:18.674854Z","iopub.execute_input":"2025-09-20T05:26:18.675205Z","iopub.status.idle":"2025-09-20T05:26:19.935951Z","shell.execute_reply.started":"2025-09-20T05:26:18.675169Z","shell.execute_reply":"2025-09-20T05:26:19.934711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Insights\nprint(df.info())\nprint(df.describe())\nprint(df['Engine_Condition'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T05:26:19.937495Z","iopub.execute_input":"2025-09-20T05:26:19.937931Z","iopub.status.idle":"2025-09-20T05:26:19.980906Z","shell.execute_reply.started":"2025-09-20T05:26:19.937898Z","shell.execute_reply":"2025-09-20T05:26:19.979921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Correlation heatmap\nplt.figure(figsize=(12,8))\nsns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T05:26:19.981652Z","iopub.execute_input":"2025-09-20T05:26:19.982001Z","iopub.status.idle":"2025-09-20T05:26:20.488666Z","shell.execute_reply.started":"2025-09-20T05:26:19.981969Z","shell.execute_reply":"2025-09-20T05:26:20.48752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Class distribution\nsns.countplot(x=\"Engine_Condition\", data=df)\nplt.title(\"Class Distribution (0=Normal, 1=Minor Fault, 2=Critical Fault)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T05:26:20.489815Z","iopub.execute_input":"2025-09-20T05:26:20.490685Z","iopub.status.idle":"2025-09-20T05:26:20.625048Z","shell.execute_reply.started":"2025-09-20T05:26:20.490637Z","shell.execute_reply":"2025-09-20T05:26:20.623925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pairplot or scatter\nsns.pairplot(df, hue=\"Engine_Condition\", diag_kind=\"kde\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T05:26:20.626411Z","iopub.execute_input":"2025-09-20T05:26:20.626674Z","iopub.status.idle":"2025-09-20T05:28:18.421882Z","shell.execute_reply.started":"2025-09-20T05:26:20.626655Z","shell.execute_reply":"2025-09-20T05:28:18.420673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3D scatter (example with vibration vs temperature vs acoustic)\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(10,7))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(df['Vibration_Amplitude'], df['Surface_Temperature'], \n           df['Acoustic_dB'], c=df['Engine_Condition'], cmap='viridis')\nax.set_xlabel('Vibration Amplitude')\nax.set_ylabel('Surface Temperature')\nax.set_zlabel('Acoustic dB')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T05:28:18.42324Z","iopub.execute_input":"2025-09-20T05:28:18.423797Z","iopub.status.idle":"2025-09-20T05:28:18.821279Z","shell.execute_reply.started":"2025-09-20T05:28:18.423751Z","shell.execute_reply":"2025-09-20T05:28:18.820375Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Preprocessing\n\nScaling for SVM/NN.\n\nTrain-Test Split.\n\nBalancing (SMOTE / class weights).","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX = df.drop(\"Engine_Condition\", axis=1)\ny = df[\"Engine_Condition\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T05:28:18.824828Z","iopub.execute_input":"2025-09-20T05:28:18.825575Z","iopub.status.idle":"2025-09-20T05:28:18.84745Z","shell.execute_reply.started":"2025-09-20T05:28:18.825536Z","shell.execute_reply":"2025-09-20T05:28:18.84628Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Baseline Model (Linear Regression)","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nlr = LogisticRegression(multi_class='multinomial', class_weight='balanced', max_iter=1000)\nlr.fit(X_train_scaled, y_train)\ny_pred = lr.predict(X_test_scaled)\n\nprint(classification_report(y_test, y_pred))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T05:28:18.848737Z","iopub.execute_input":"2025-09-20T05:28:18.849011Z","iopub.status.idle":"2025-09-20T05:28:19.348541Z","shell.execute_reply.started":"2025-09-20T05:28:18.84899Z","shell.execute_reply":"2025-09-20T05:28:19.347519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Tree-Based Models\n\nRandom Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(class_weight='balanced', random_state=42)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap=\"Greens\")\n\n# Feature importance\nfeat_imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\nfeat_imp.plot(kind='bar', figsize=(10,6), title=\"Feature Importance\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T05:28:19.349642Z","iopub.execute_input":"2025-09-20T05:28:19.349948Z","iopub.status.idle":"2025-09-20T05:28:24.191138Z","shell.execute_reply.started":"2025-09-20T05:28:19.349922Z","shell.execute_reply":"2025-09-20T05:28:24.19024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"XGBoost","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', scale_pos_weight=1)\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap=\"Oranges\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T05:28:24.192161Z","iopub.execute_input":"2025-09-20T05:28:24.192537Z","iopub.status.idle":"2025-09-20T05:28:25.299545Z","shell.execute_reply.started":"2025-09-20T05:28:24.192506Z","shell.execute_reply":"2025-09-20T05:28:25.298618Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model Training & Comparison","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n\n# Dictionary to store results\nresults = {}\n\n# Logistic Regression\nlr = LogisticRegression(multi_class='multinomial', class_weight='balanced', max_iter=1000, random_state=42)\nlr.fit(X_train_scaled, y_train)\ny_pred_lr = lr.predict(X_test_scaled)\nresults['Logistic Regression'] = {\n    'Accuracy': accuracy_score(y_test, y_pred_lr),\n    'Precision': precision_score(y_test, y_pred_lr, average='weighted'),\n    'Recall': recall_score(y_test, y_pred_lr, average='weighted'),\n    'F1-score': f1_score(y_test, y_pred_lr, average='weighted')\n}\n\n# Random Forest\nrf = RandomForestClassifier(class_weight='balanced', random_state=42)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\nresults['Random Forest'] = {\n    'Accuracy': accuracy_score(y_test, y_pred_rf),\n    'Precision': precision_score(y_test, y_pred_rf, average='weighted'),\n    'Recall': recall_score(y_test, y_pred_rf, average='weighted'),\n    'F1-score': f1_score(y_test, y_pred_rf, average='weighted')\n}\n\n# XGBoost\nxgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\nresults['XGBoost'] = {\n    'Accuracy': accuracy_score(y_test, y_pred_xgb),\n    'Precision': precision_score(y_test, y_pred_xgb, average='weighted'),\n    'Recall': recall_score(y_test, y_pred_xgb, average='weighted'),\n    'F1-score': f1_score(y_test, y_pred_xgb, average='weighted')\n}\n\n# Support Vector Machine\nsvm = SVC(kernel='rbf', class_weight='balanced', random_state=42)\nsvm.fit(X_train_scaled, y_train)\ny_pred_svm = svm.predict(X_test_scaled)\nresults['SVM (RBF)'] = {\n    'Accuracy': accuracy_score(y_test, y_pred_svm),\n    'Precision': precision_score(y_test, y_pred_svm, average='weighted'),\n    'Recall': recall_score(y_test, y_pred_svm, average='weighted'),\n    'F1-score': f1_score(y_test, y_pred_svm, average='weighted')\n}\n\n# Convert results to DataFrame for comparison\ncomparison_df = pd.DataFrame(results).T\ncomparison_df = comparison_df.sort_values(by=\"F1-score\", ascending=False)\nprint(comparison_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T05:28:25.300451Z","iopub.execute_input":"2025-09-20T05:28:25.300704Z","iopub.status.idle":"2025-09-20T05:28:35.849882Z","shell.execute_reply.started":"2025-09-20T05:28:25.300683Z","shell.execute_reply":"2025-09-20T05:28:35.84888Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualization of Comparison","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.barplot(x=comparison_df.index, y=comparison_df[\"F1-score\"])\nplt.title(\"Model Comparison (F1-score)\")\nplt.ylabel(\"F1-score\")\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T05:28:35.850921Z","iopub.execute_input":"2025-09-20T05:28:35.851178Z","iopub.status.idle":"2025-09-20T05:28:36.008064Z","shell.execute_reply.started":"2025-09-20T05:28:35.851156Z","shell.execute_reply":"2025-09-20T05:28:36.006614Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Observations from Your Results\n\nRandom Forest (59.6% Accuracy) is currently the best.\n\nXGBoost (55.9%) is slightly behind but has better balance between recall and F1.\n\nSVM & Logistic Regression are performing poorly (expected due to non-linear dataset & imbalance).\n\nOverall, accuracy < 60% → models are not learning patterns strongly enough.\n\n2. Why Performance is Low\n\nImbalanced Dataset:\n\nNormal (60%), Minor fault (30%), Critical fault (10%).\n\nModels tend to predict the majority class (\"Normal\").\n\nThat’s why Recall and F1 are low for minority classes.\n\nComplex Feature Interactions:\n\nThe engine fault patterns may not be linearly separable.\n\nRandom Forest helps, but feature tuning is needed.\n\nDefault Hyperparameters:\n\nThe models you used are with default settings.\n\nThey usually underperform until tuned.","metadata":{}},{"cell_type":"markdown","source":"A. Handle Class Imbalance\n\nUse SMOTE (Synthetic Minority Oversampling Technique) for training set.\n\nOr use class_weight='balanced' (you already used this in some models, but SMOTE often works better).","metadata":{}},{"cell_type":"code","source":"print(df.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T05:29:59.13754Z","iopub.execute_input":"2025-09-20T05:29:59.137863Z","iopub.status.idle":"2025-09-20T05:29:59.143294Z","shell.execute_reply.started":"2025-09-20T05:29:59.137824Z","shell.execute_reply":"2025-09-20T05:29:59.142077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# 1. Split features and target\nX = df.drop('Engine_Condition', axis=1)  # Replace 'target' with the actual target column name\ny = df['Engine_Condition']\n\n# 2. Train-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\n# 3. Scale features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 4. Apply SMOTE to training data only\nsm = SMOTE(random_state=42)\nX_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)\n\n# Optional: Visualize class balance after SMOTE\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.countplot(x=y_train_res)\nplt.title(\"Class Distribution After SMOTE\")\nplt.show()\n\n# 5. Define models\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nmodels = {\n    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"SVM (RBF)\": SVC(kernel='rbf', probability=True),\n    \"Logistic Regression\": LogisticRegression(max_iter=1000)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T05:33:07.989656Z","iopub.execute_input":"2025-09-20T05:33:07.98998Z","iopub.status.idle":"2025-09-20T05:33:08.325586Z","shell.execute_reply.started":"2025-09-20T05:33:07.98996Z","shell.execute_reply":"2025-09-20T05:33:08.3246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators': [100, 300, 500],\n    'max_depth': [5, 10, 20, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'class_weight': ['balanced']\n}\n\ngrid_rf = GridSearchCV(RandomForestClassifier(random_state=42),\n                       param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\ngrid_rf.fit(X_train_res, y_train_res)\n\nprint(\"Best RF params:\", grid_rf.best_params_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:07:21.188509Z","iopub.execute_input":"2025-09-20T06:07:21.188947Z","iopub.status.idle":"2025-09-20T06:38:29.414523Z","shell.execute_reply.started":"2025-09-20T06:07:21.188917Z","shell.execute_reply":"2025-09-20T06:38:29.413285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Reuse models (re-define if necessary)\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nmodels_after_smote = {\n    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n    \n    # Use best Random Forest parameters from your GridSearch\n    \"Random Forest\": RandomForestClassifier(\n        class_weight='balanced',\n        max_depth=None,\n        min_samples_leaf=1,\n        min_samples_split=2,\n        n_estimators=500,\n        random_state=42\n    ),\n    \n    \"SVM (RBF)\": SVC(kernel='rbf', probability=True),\n    \"Logistic Regression\": LogisticRegression(max_iter=1000)\n}\n\n# Evaluate\nresults_after_smote = []\n\nfor name, model in models_after_smote.items():\n    model.fit(X_train_res, y_train_res)\n    y_pred = model.predict(X_test_scaled)\n    \n    results_after_smote.append({\n        \"Model\": name,\n        \"Accuracy\": accuracy_score(y_test, y_pred),\n        \"Precision\": precision_score(y_test, y_pred, average='weighted'),\n        \"Recall\": recall_score(y_test, y_pred, average='weighted'),\n        \"F1-score\": f1_score(y_test, y_pred, average='weighted')\n    })\n\n# Show results\nimport pandas as pd\nresults_df_smote = pd.DataFrame(results_after_smote)\nprint(\" Evaluation AFTER SMOTE:\")\nprint(results_df_smote)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:45:29.788968Z","iopub.execute_input":"2025-09-20T06:45:29.789899Z","iopub.status.idle":"2025-09-20T06:47:17.768071Z","shell.execute_reply.started":"2025-09-20T06:45:29.789869Z","shell.execute_reply":"2025-09-20T06:47:17.767124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nfor name, model in models_after_smote.items():\n    y_pred = model.predict(X_test_scaled)\n    cm = confusion_matrix(y_test, y_pred)\n    print(f\"\\n{name} Confusion Matrix:\")\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot()\n    plt.title(f\"{name} Confusion Matrix After SMOTE\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:49:12.597914Z","iopub.execute_input":"2025-09-20T06:49:12.598243Z","iopub.status.idle":"2025-09-20T06:49:14.832292Z","shell.execute_reply.started":"2025-09-20T06:49:12.598214Z","shell.execute_reply":"2025-09-20T06:49:14.83113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.combine import SMOTEENN, SMOTETomek\n\nsmote_enn = SMOTEENN(random_state=42)\nX_train_res2, y_train_res2 = smote_enn.fit_resample(X_train_scaled, y_train)\n\n# Repeat model training using X_train_res2 and y_train_res2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:49:36.502387Z","iopub.execute_input":"2025-09-20T06:49:36.502709Z","iopub.status.idle":"2025-09-20T06:49:38.076582Z","shell.execute_reply.started":"2025-09-20T06:49:36.502688Z","shell.execute_reply":"2025-09-20T06:49:38.075613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nfor name, model in models_after_smote.items():\n    y_pred = model.predict(X_test_scaled)\n    print(f\"\\n{name} Classification Report:\")\n    print(classification_report(y_test, y_pred, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:49:55.137353Z","iopub.execute_input":"2025-09-20T06:49:55.137625Z","iopub.status.idle":"2025-09-20T06:49:56.701791Z","shell.execute_reply.started":"2025-09-20T06:49:55.137607Z","shell.execute_reply":"2025-09-20T06:49:56.700792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(\n    class_weight='balanced',\n    n_estimators=500,\n    max_depth=10,\n    learning_rate=0.05,\n    random_state=42\n)\nlgbm.fit(X_train_res, y_train_res)\ny_pred = lgbm.predict(X_test_scaled)\nprint(classification_report(y_test, y_pred, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:51:55.658599Z","iopub.execute_input":"2025-09-20T06:51:55.658926Z","iopub.status.idle":"2025-09-20T06:52:03.095637Z","shell.execute_reply.started":"2025-09-20T06:51:55.658905Z","shell.execute_reply":"2025-09-20T06:52:03.094635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.combine import SMOTETomek\n\nsmote_tomek = SMOTETomek(random_state=42)\nX_train_res2, y_train_res2 = smote_tomek.fit_resample(X_train_scaled, y_train)\n\n# Train best model again (e.g., Random Forest, LightGBM)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:52:16.14231Z","iopub.execute_input":"2025-09-20T06:52:16.142866Z","iopub.status.idle":"2025-09-20T06:52:17.115566Z","shell.execute_reply.started":"2025-09-20T06:52:16.142826Z","shell.execute_reply":"2025-09-20T06:52:17.114676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"importances = grid_rf.best_estimator_.feature_importances_\nfeature_names = X.columns\nplt.barh(feature_names, importances)\nplt.title(\"Feature Importances - Random Forest\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:52:35.352549Z","iopub.execute_input":"2025-09-20T06:52:35.3528Z","iopub.status.idle":"2025-09-20T06:52:35.566749Z","shell.execute_reply.started":"2025-09-20T06:52:35.352783Z","shell.execute_reply":"2025-09-20T06:52:35.565887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\ncat = CatBoostClassifier(\n    iterations=500,\n    depth=6,\n    learning_rate=0.05,\n    loss_function='MultiClass',\n    class_weights=[1, 2, 6],  # Tune this based on class distribution\n    verbose=0,\n    random_state=42\n)\n\ncat.fit(X_train_scaled, y_train)\ny_pred = cat.predict(X_test_scaled)\nprint(classification_report(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:53:29.451565Z","iopub.execute_input":"2025-09-20T06:53:29.451945Z","iopub.status.idle":"2025-09-20T06:53:34.021823Z","shell.execute_reply.started":"2025-09-20T06:53:29.451921Z","shell.execute_reply":"2025-09-20T06:53:34.020731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['binary_target'] = df['Engine_Condition'].apply(lambda x: 1 if x == 2 else 0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:54:15.190589Z","iopub.execute_input":"2025-09-20T06:54:15.190896Z","iopub.status.idle":"2025-09-20T06:54:15.199071Z","shell.execute_reply.started":"2025-09-20T06:54:15.190873Z","shell.execute_reply":"2025-09-20T06:54:15.198037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nfrom sklearn.metrics import classification_report\n\n# Assign class weights (you can tune these)\nclass_weights = [1.0, 2.0, 6.0]  # More weight to rare class\n\ncat = CatBoostClassifier(\n    iterations=500,\n    learning_rate=0.05,\n    depth=6,\n    loss_function='MultiClass',\n    class_weights=class_weights,\n    random_state=42,\n    verbose=100\n)\n\ncat.fit(X_train_scaled, y_train)\ny_pred = cat.predict(X_test_scaled)\nprint(classification_report(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:54:23.261283Z","iopub.execute_input":"2025-09-20T06:54:23.26216Z","iopub.status.idle":"2025-09-20T06:54:27.167454Z","shell.execute_reply.started":"2025-09-20T06:54:23.262126Z","shell.execute_reply":"2025-09-20T06:54:27.166592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shap\n\nexplainer = shap.TreeExplainer(cat)\nshap_values = explainer.shap_values(X_train_scaled)\n\nshap.summary_plot(shap_values, X_train_scaled, feature_names=X.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:54:39.800277Z","iopub.execute_input":"2025-09-20T06:54:39.800552Z","iopub.status.idle":"2025-09-20T06:54:49.068759Z","shell.execute_reply.started":"2025-09-20T06:54:39.800534Z","shell.execute_reply":"2025-09-20T06:54:49.067453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['binary_target'] = df['Engine_Condition'].apply(lambda x: 1 if x == 2 else 0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:55:16.672137Z","iopub.execute_input":"2025-09-20T06:55:16.672723Z","iopub.status.idle":"2025-09-20T06:55:16.681022Z","shell.execute_reply.started":"2025-09-20T06:55:16.6727Z","shell.execute_reply":"2025-09-20T06:55:16.679991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Prepare features and new binary target\nX = df.drop(['Engine_Condition', 'binary_target'], axis=1)\ny = df['binary_target']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, test_size=0.2, random_state=42\n)\n\n# Scale\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# CatBoost with class weights\nmodel = CatBoostClassifier(\n    iterations=500,\n    learning_rate=0.05,\n    depth=6,\n    class_weights=[1, 6],  # boost the importance of detecting class 1 (critical fault)\n    verbose=100,\n    random_state=42\n)\n\nmodel.fit(X_train_scaled, y_train)\ny_pred = model.predict(X_test_scaled)\n\n# Evaluation\nprint(classification_report(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:55:24.932Z","iopub.execute_input":"2025-09-20T06:55:24.932354Z","iopub.status.idle":"2025-09-20T06:55:27.62169Z","shell.execute_reply.started":"2025-09-20T06:55:24.932313Z","shell.execute_reply":"2025-09-20T06:55:27.620322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode labels and create categorical target\nencoder = LabelEncoder()\ny_enc = encoder.fit_transform(y_train_res)\ny_cat = to_categorical(y_enc)\n\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train_res.shape[1],)),\n    BatchNormalization(),\n    Dropout(0.3),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(3, activation='softmax')\n])\n\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(X_train_res, y_cat, epochs=50, batch_size=64, validation_split=0.2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:57:17.070076Z","iopub.execute_input":"2025-09-20T06:57:17.070388Z","iopub.status.idle":"2025-09-20T06:58:10.353703Z","shell.execute_reply.started":"2025-09-20T06:57:17.070366Z","shell.execute_reply":"2025-09-20T06:58:10.352877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\ny_train_onehot = to_categorical(y_train, num_classes=3)\ny_val_onehot = to_categorical(y_val, num_classes=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:59:53.504019Z","iopub.execute_input":"2025-09-20T06:59:53.504805Z","iopub.status.idle":"2025-09-20T06:59:53.510741Z","shell.execute_reply.started":"2025-09-20T06:59:53.504776Z","shell.execute_reply":"2025-09-20T06:59:53.509909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train_onehot, epochs=50, batch_size=64, validation_data=(X_val, y_val_onehot), class_weight=class_weight_dict)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:00:00.843731Z","iopub.execute_input":"2025-09-20T07:00:00.844054Z","iopub.status.idle":"2025-09-20T07:00:29.323072Z","shell.execute_reply.started":"2025-09-20T07:00:00.844034Z","shell.execute_reply":"2025-09-20T07:00:29.321927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val), class_weight=class_weight_dict)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:00:33.7774Z","iopub.execute_input":"2025-09-20T07:00:33.777695Z","iopub.status.idle":"2025-09-20T07:01:00.109991Z","shell.execute_reply.started":"2025-09-20T07:00:33.777674Z","shell.execute_reply":"2025-09-20T07:01:00.108917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Required libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Split dataset (assuming X and y are your features and labels)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Random Forest Model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\ny_pred_rf = rf_model.predict(X_test)\nrf_accuracy = accuracy_score(y_test, y_pred_rf)\nprint(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n\n# XGBoost Model\nxgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\nxgb_model.fit(X_train, y_train)\ny_pred_xgb = xgb_model.predict(X_test)\nxgb_accuracy = accuracy_score(y_test, y_pred_xgb)\nprint(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:04:45.723523Z","iopub.execute_input":"2025-09-20T07:04:45.723972Z","iopub.status.idle":"2025-09-20T07:04:50.717029Z","shell.execute_reply.started":"2025-09-20T07:04:45.723951Z","shell.execute_reply":"2025-09-20T07:04:50.715928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, classification_report\n\n# For binary classification; if multiclass, adjust average parameter accordingly\naverage_type = 'binary'  # or 'weighted' or 'macro' if multiclass\n\n# Random Forest metrics\nprecision_rf = precision_score(y_test, y_pred_rf, average=average_type)\nrecall_rf = recall_score(y_test, y_pred_rf, average=average_type)\nf1_rf = f1_score(y_test, y_pred_rf, average=average_type)\nconf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\nprint(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\nprint(\"Confusion Matrix:\\n\", conf_matrix_rf)\n\n# XGBoost metrics\nprecision_xgb = precision_score(y_test, y_pred_xgb, average=average_type)\nrecall_xgb = recall_score(y_test, y_pred_xgb, average=average_type)\nf1_xgb = f1_score(y_test, y_pred_xgb, average=average_type)\nconf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\nprint(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_pred_xgb))\nprint(\"Confusion Matrix:\\n\", conf_matrix_xgb)\n\n# If binary classification, ROC-AUC can be calculated as:\nif average_type == 'binary':\n    # Get probabilities for the positive class\n    y_prob_rf = rf_model.predict_proba(X_test)[:, 1]\n    y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]\n\n    roc_auc_rf = roc_auc_score(y_test, y_prob_rf)\n    roc_auc_xgb = roc_auc_score(y_test, y_prob_xgb)\n    print(f\"Random Forest ROC-AUC: {roc_auc_rf:.4f}\")\n    print(f\"XGBoost ROC-AUC: {roc_auc_xgb:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:05:38.450634Z","iopub.execute_input":"2025-09-20T07:05:38.450967Z","iopub.status.idle":"2025-09-20T07:05:38.549896Z","shell.execute_reply.started":"2025-09-20T07:05:38.450945Z","shell.execute_reply":"2025-09-20T07:05:38.548952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:06:24.452439Z","iopub.execute_input":"2025-09-20T07:06:24.452711Z","iopub.status.idle":"2025-09-20T07:06:24.471698Z","shell.execute_reply.started":"2025-09-20T07:06:24.452693Z","shell.execute_reply":"2025-09-20T07:06:24.470919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(class_weight='balanced', random_state=42)\nrf.fit(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:06:30.903981Z","iopub.execute_input":"2025-09-20T07:06:30.90428Z","iopub.status.idle":"2025-09-20T07:06:34.781215Z","shell.execute_reply.started":"2025-09-20T07:06:30.904259Z","shell.execute_reply":"2025-09-20T07:06:34.779938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate class distribution counts\nfrom collections import Counter\n\ncounter = Counter(y_train)\nnumber_of_majority = counter[0]  # Assuming '0' is the majority class label\nnumber_of_minority = counter[1]  # Assuming '1' is the minority class label\n\nprint(f\"Majority class count: {number_of_majority}\")\nprint(f\"Minority class count: {number_of_minority}\")\n\n# Now use scale_pos_weight in XGBoost\nimport xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(scale_pos_weight=number_of_majority / number_of_minority, random_state=42)\nxgb_model.fit(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:07:09.029268Z","iopub.execute_input":"2025-09-20T07:07:09.029556Z","iopub.status.idle":"2025-09-20T07:07:09.258275Z","shell.execute_reply.started":"2025-09-20T07:07:09.029537Z","shell.execute_reply":"2025-09-20T07:07:09.257447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\n\n# Get counts for each class\nclass_counts = Counter(y_train)\ntotal = sum(class_counts.values())\n\n# Calculate class weights: total_samples / (num_classes * class_count)\nnum_classes = 3\nclass_weights = {cls: total / (num_classes * count) for cls, count in class_counts.items()}\n\n# Create sample weights for each instance in y_train\nsample_weights = np.array([class_weights[label] for label in y_train])\n\n# Train model with sample weights\nxgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=num_classes, random_state=42)\nxgb_model.fit(X_train, y_train, sample_weight=sample_weights)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:07:53.052047Z","iopub.execute_input":"2025-09-20T07:07:53.052368Z","iopub.status.idle":"2025-09-20T07:07:53.543871Z","shell.execute_reply.started":"2025-09-20T07:07:53.052349Z","shell.execute_reply":"2025-09-20T07:07:53.54298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclass_weights = {0: 1, 1: 9, 2: 5}  # example weights, calculate based on your data\n\nrf = RandomForestClassifier(class_weight=class_weights, random_state=42)\nrf.fit(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:08:10.940422Z","iopub.execute_input":"2025-09-20T07:08:10.940699Z","iopub.status.idle":"2025-09-20T07:08:15.008516Z","shell.execute_reply.started":"2025-09-20T07:08:10.940681Z","shell.execute_reply":"2025-09-20T07:08:15.007583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Define class weights based on imbalance\nclass_weights = {0: 1, 1: 9, 2: 5}  # adjust weights based on your data distribution\n\n# Initialize model with class weights\nrf = RandomForestClassifier(class_weight=class_weights, random_state=42)\n\n# Train\nrf.fit(X_train, y_train)\n\n# Predict\ny_pred = rf.predict(X_test)\n\n# Evaluate\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:08:37.24516Z","iopub.execute_input":"2025-09-20T07:08:37.245469Z","iopub.status.idle":"2025-09-20T07:08:41.118446Z","shell.execute_reply.started":"2025-09-20T07:08:37.245448Z","shell.execute_reply":"2025-09-20T07:08:41.117292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\ncounts = Counter(y_train)\ntotal = sum(counts.values())\nnum_classes = len(counts)\n\nweights = {cls: total / (num_classes * count) for cls, count in counts.items()}\nprint(weights)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:08:59.291588Z","iopub.execute_input":"2025-09-20T07:08:59.291891Z","iopub.status.idle":"2025-09-20T07:08:59.298961Z","shell.execute_reply.started":"2025-09-20T07:08:59.291869Z","shell.execute_reply":"2025-09-20T07:08:59.29786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom collections import Counter\n\n# Check original distribution\nprint(\"Before SMOTE:\", Counter(y_train))\n\n# Apply SMOTE to balance dataset\nsmote = SMOTE(random_state=42)\nX_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n\nprint(\"After SMOTE:\", Counter(y_train_res))\n\n# Train with balanced data\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train_res, y_train_res)\n\n# Predict and evaluate\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:09:07.627989Z","iopub.execute_input":"2025-09-20T07:09:07.628262Z","iopub.status.idle":"2025-09-20T07:09:17.94604Z","shell.execute_reply.started":"2025-09-20T07:09:07.628243Z","shell.execute_reply":"2025-09-20T07:09:17.944895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [10, 20, None],\n    'min_samples_split': [2, 5],\n    'class_weight': [{0:1, 1:5}, {0:1, 1:10}, 'balanced']\n}\n\nrf = RandomForestClassifier(random_state=42)\n\ngrid_search = GridSearchCV(rf, param_grid, scoring='f1', cv=3, n_jobs=-1)\ngrid_search.fit(X_train_res, y_train_res)\n\nprint(\"Best parameters:\", grid_search.best_params_)\nbest_rf = grid_search.best_estimator_\n\ny_pred = best_rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:09:33.572653Z","iopub.execute_input":"2025-09-20T07:09:33.572985Z","iopub.status.idle":"2025-09-20T07:17:05.691707Z","shell.execute_reply.started":"2025-09-20T07:09:33.572964Z","shell.execute_reply":"2025-09-20T07:17:05.690758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom imblearn.over_sampling import SMOTE\nimport xgboost as xgb\nfrom sklearn.preprocessing import label_binarize\n\n# Assuming your data is in X and y, with 3 classes: 0, 1, 2\n\n# 1. Train-test split (stratified)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 2. Apply SMOTE to training data\nsmote = SMOTE(random_state=42)\nX_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n\nprint(\"Before SMOTE:\", Counter(y_train))\nprint(\"After SMOTE:\", Counter(y_train_res))\n\n# 3. Random Forest with best hyperparameters\nrf_model = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=None,\n    min_samples_split=2,\n    class_weight='balanced',\n    random_state=42\n)\nrf_model.fit(X_train_res, y_train_res)\ny_pred_rf = rf_model.predict(X_test)\ny_proba_rf = rf_model.predict_proba(X_test)\n\nprint(\"Random Forest Classification Report:\")\nprint(classification_report(y_test, y_pred_rf))\n\nprint(\"Random Forest Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred_rf))\n\n# 4. Calculate multiclass ROC-AUC (One-vs-Rest)\n# Binarize the output labels for multi-class ROC AUC\ny_test_bin = label_binarize(y_test, classes=[0, 1, 2])\nroc_auc_rf = roc_auc_score(y_test_bin, y_proba_rf, multi_class='ovr')\nprint(f\"Random Forest ROC-AUC (OvR): {roc_auc_rf:.4f}\")\n\n# 5. XGBoost with SMOTE data\nxgb_model = xgb.XGBClassifier(\n    n_estimators=200,\n    max_depth=6,\n    learning_rate=0.1,\n    use_label_encoder=False,\n    eval_metric='mlogloss',\n    random_state=42\n)\nxgb_model.fit(X_train_res, y_train_res)\ny_pred_xgb = xgb_model.predict(X_test)\ny_proba_xgb = xgb_model.predict_proba(X_test)\n\nprint(\"\\nXGBoost Classification Report:\")\nprint(classification_report(y_test, y_pred_xgb))\n\nprint(\"XGBoost Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred_xgb))\n\nroc_auc_xgb = roc_auc_score(y_test_bin, y_proba_xgb, multi_class='ovr')\nprint(f\"XGBoost ROC-AUC (OvR): {roc_auc_xgb:.4f}\")\n\n# 6. Final comparison\nprint(\"\\nFinal Model Comparison:\")\nprint(f\"Random Forest ROC-AUC: {roc_auc_rf:.4f}\")\nprint(f\"XGBoost ROC-AUC: {roc_auc_xgb:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:22:04.455686Z","iopub.execute_input":"2025-09-20T07:22:04.456662Z","iopub.status.idle":"2025-09-20T07:22:25.098427Z","shell.execute_reply.started":"2025-09-20T07:22:04.456628Z","shell.execute_reply":"2025-09-20T07:22:25.097229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\n# Assume X, y are your dataset features and labels (binary classification: classes 0 and 1)\n\n# 1. Train-test split (stratified)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2, \n    random_state=42, \n    stratify=y\n)\n\n# 2. Apply SMOTEENN on training data\nsmote_enn = SMOTEENN(random_state=42)\nX_train_res, y_train_res = smote_enn.fit_resample(X_train, y_train)\n\nprint(\"Before SMOTEENN:\", Counter(y_train))\nprint(\"After SMOTEENN:\", Counter(y_train_res))\n\n# 3. Train Balanced Random Forest\nbrf = BalancedRandomForestClassifier(n_estimators=200, random_state=42)\nbrf.fit(X_train_res, y_train_res)\n\n# 4. Predict probabilities on test set\ny_proba = brf.predict_proba(X_test)\n\n# 5. Tune threshold for minority classes (here for class 1)\noptimal_thresholds = {}\nfor class_idx in np.unique(y_test):\n    precision, recall, thresholds = precision_recall_curve((y_test == class_idx).astype(int), y_proba[:, class_idx])\n    f1_scores = 2 * recall * precision / (recall + precision + 1e-6)\n    best_idx = np.argmax(f1_scores)\n    # If best_idx == len(thresholds), use 0.5 as fallback threshold\n    optimal_thresholds[class_idx] = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n\nprint(\"Optimal thresholds per class:\", optimal_thresholds)\n\n# 6. Predict classes with tuned thresholds\ny_pred_tuned = []\nfor prob in y_proba:\n    class_preds = []\n    for cls in range(len(optimal_thresholds)):\n        class_preds.append(prob[cls] >= optimal_thresholds[cls])\n    if sum(class_preds) == 1:\n        y_pred_tuned.append(class_preds.index(True))\n    else:\n        # If multiple or none meet threshold, pick the class with max probability\n        y_pred_tuned.append(np.argmax(prob))\ny_pred_tuned = np.array(y_pred_tuned)\n\n# 7. Evaluation\nprint(\"Balanced Random Forest Classification Report with Threshold Tuning:\")\nprint(classification_report(y_test, y_pred_tuned))\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred_tuned))\n\n# 8. Correct ROC-AUC for binary classification\nroc_auc = roc_auc_score(y_test, y_proba[:, 1])\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:30:03.847028Z","iopub.execute_input":"2025-09-20T07:30:03.848971Z","iopub.status.idle":"2025-09-20T07:30:14.637464Z","shell.execute_reply.started":"2025-09-20T07:30:03.848937Z","shell.execute_reply":"2025-09-20T07:30:14.636035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nimport lightgbm as lgb\n\n# 1. Train-test split (stratified)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 2. Apply SMOTEENN on training data\nsmote_enn = SMOTEENN(random_state=42)\nX_train_res, y_train_res = smote_enn.fit_resample(X_train, y_train)\n\nprint(\"Before SMOTEENN:\", Counter(y_train))\nprint(\"After SMOTEENN:\", Counter(y_train_res))\n\n# --- Balanced Random Forest ---\nbrf = BalancedRandomForestClassifier(n_estimators=200, random_state=42)\nbrf.fit(X_train_res, y_train_res)\ny_proba_brf = brf.predict_proba(X_test)\n\n# Threshold tuning for BRF\noptimal_thresholds_brf = {}\nfor class_idx in np.unique(y_test):\n    precision, recall, thresholds = precision_recall_curve((y_test == class_idx).astype(int), y_proba_brf[:, class_idx])\n    f1_scores = 2 * precision * recall / (precision + recall + 1e-6)\n    best_idx = np.argmax(f1_scores)\n    optimal_thresholds_brf[class_idx] = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n\n# Predict with tuned threshold BRF\ny_pred_brf = []\nfor prob in y_proba_brf:\n    preds = [prob[cls] >= optimal_thresholds_brf[cls] for cls in range(len(optimal_thresholds_brf))]\n    if sum(preds) == 1:\n        y_pred_brf.append(preds.index(True))\n    else:\n        y_pred_brf.append(np.argmax(prob))\ny_pred_brf = np.array(y_pred_brf)\n\nprint(\"\\nBalanced Random Forest Classification Report:\")\nprint(classification_report(y_test, y_pred_brf))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_brf))\nprint(f\"Balanced RF ROC-AUC: {roc_auc_score(y_test, y_proba_brf[:,1]):.4f}\")\n\n# --- LightGBM with class weights ---\n# Calculate class weights inversely proportional to class frequency\nclass_weights = {cls: 1.0/count for cls, count in Counter(y_train).items()}\nweights = np.array([class_weights[label] for label in y_train])\n\nlgb_train = lgb.Dataset(X_train, label=y_train, weight=weights)\nparams = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'verbosity': -1,\n    'seed': 42\n}\n\nlgb_model = lgb.train(params, lgb_train, num_boost_round=200)\n\n# Predict probabilities on test set\ny_proba_lgb = lgb_model.predict(X_test)\n\n# Threshold tuning for LightGBM\nprecision, recall, thresholds = precision_recall_curve(y_test, y_proba_lgb)\nf1_scores = 2 * precision * recall / (precision + recall + 1e-6)\nbest_idx = np.argmax(f1_scores)\noptimal_threshold_lgb = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n\ny_pred_lgb = (y_proba_lgb >= optimal_threshold_lgb).astype(int)\n\nprint(\"\\nLightGBM Classification Report:\")\nprint(classification_report(y_test, y_pred_lgb))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lgb))\nprint(f\"LightGBM ROC-AUC: {roc_auc_score(y_test, y_proba_lgb):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:30:58.006558Z","iopub.execute_input":"2025-09-20T07:30:58.006868Z","iopub.status.idle":"2025-09-20T07:31:09.080208Z","shell.execute_reply.started":"2025-09-20T07:30:58.006826Z","shell.execute_reply":"2025-09-20T07:31:09.079133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n\n# Assuming X_train, y_train, weights are pandas DataFrame/Series and numpy array\n\nparams = {\n    'objective': 'multiclass',\n    'num_class': len(np.unique(y_train)),\n    'metric': 'multi_logloss',\n    'verbosity': -1,   # suppress logs\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.1,\n    'num_leaves': 31,\n    'seed': 42\n}\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nmodels = []\nbest_iteration = 0\n\nfor train_idx, val_idx in skf.split(X_train, y_train):\n    lgb_train_cv = lgb.Dataset(X_train.iloc[train_idx], \n                               label=y_train.iloc[train_idx], \n                               weight=weights[train_idx])\n    lgb_val_cv = lgb.Dataset(X_train.iloc[val_idx], \n                             label=y_train.iloc[val_idx], \n                             weight=weights[val_idx])\n    \n    model = lgb.train(\n        params,\n        lgb_train_cv,\n        valid_sets=[lgb_val_cv],\n        callbacks=[lgb.early_stopping(stopping_rounds=50)]\n    )\n    \n    best_iteration = max(best_iteration, model.best_iteration)\n    models.append(model)\n\nprint(f\"Best iteration across folds: {best_iteration}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:33:00.158336Z","iopub.execute_input":"2025-09-20T07:33:00.158686Z","iopub.status.idle":"2025-09-20T07:33:00.939961Z","shell.execute_reply.started":"2025-09-20T07:33:00.158666Z","shell.execute_reply":"2025-09-20T07:33:00.939116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    'objective': 'multiclass',\n    'num_class': len(np.unique(y_train)),\n    'metric': 'multi_logloss',\n    'verbosity': -1,\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.05,    # lowered learning rate\n    'num_leaves': 50,         # increased complexity\n    'seed': 42\n}\n\nnum_boost_round = 500\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nmodels = []\nbest_iteration = 0\n\nfor train_idx, val_idx in skf.split(X_train, y_train):\n    lgb_train_cv = lgb.Dataset(X_train.iloc[train_idx], \n                               label=y_train.iloc[train_idx], \n                               weight=weights[train_idx])\n    lgb_val_cv = lgb.Dataset(X_train.iloc[val_idx], \n                             label=y_train.iloc[val_idx], \n                             weight=weights[val_idx])\n    \n    model = lgb.train(\n        params,\n        lgb_train_cv,\n        valid_sets=[lgb_val_cv],\n        num_boost_round=num_boost_round,\n        callbacks=[lgb.early_stopping(stopping_rounds=50)]\n    )\n    \n    best_iteration = max(best_iteration, model.best_iteration)\n    models.append(model)\n\nprint(f\"Best iteration across folds: {best_iteration}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:33:23.924916Z","iopub.execute_input":"2025-09-20T07:33:23.925714Z","iopub.status.idle":"2025-09-20T07:33:25.114945Z","shell.execute_reply.started":"2025-09-20T07:33:23.925678Z","shell.execute_reply":"2025-09-20T07:33:25.113956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    'objective': 'multiclass',\n    'num_class': len(np.unique(y_train)),\n    'metric': 'multi_logloss',\n    'verbosity': -1,\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.01,    # smaller learning rate\n    'num_leaves': 100,        # more complex trees\n    'max_depth': 10,          # deeper trees\n    'seed': 42,\n    'is_unbalance': True      # handle imbalance inside LightGBM\n}\n\nnum_boost_round = 1000\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nmodels = []\nbest_iteration = 0\n\nfor train_idx, val_idx in skf.split(X_train, y_train):\n    lgb_train_cv = lgb.Dataset(X_train.iloc[train_idx], label=y_train.iloc[train_idx])\n    lgb_val_cv = lgb.Dataset(X_train.iloc[val_idx], label=y_train.iloc[val_idx])\n    \n    model = lgb.train(\n        params,\n        lgb_train_cv,\n        valid_sets=[lgb_val_cv],\n        num_boost_round=num_boost_round,\n        callbacks=[lgb.early_stopping(stopping_rounds=50)]\n    )\n    \n    best_iteration = max(best_iteration, model.best_iteration)\n    models.append(model)\n\nprint(f\"Best iteration across folds: {best_iteration}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:33:41.542369Z","iopub.execute_input":"2025-09-20T07:33:41.542658Z","iopub.status.idle":"2025-09-20T07:33:43.626885Z","shell.execute_reply.started":"2025-09-20T07:33:41.542638Z","shell.execute_reply":"2025-09-20T07:33:43.625882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_model = lgb.train(\n    params,\n    lgb.Dataset(X_train, label=y_train),\n    num_boost_round=48\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:36:02.362657Z","iopub.execute_input":"2025-09-20T07:36:02.362974Z","iopub.status.idle":"2025-09-20T07:36:02.500325Z","shell.execute_reply.started":"2025-09-20T07:36:02.362955Z","shell.execute_reply":"2025-09-20T07:36:02.499435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\nfrom imblearn.combine import SMOTEENN\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nimport lightgbm as lgb\n\n# Generate imbalanced classification data\nX, y = make_classification(n_samples=10000, n_features=20, weights=[0.9, 0.1], random_state=42)\n\nprint(\"Original class distribution:\", Counter(y))\n\n# Split train-test\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\n# Apply SMOTEENN to balance training data\nsmote_enn = SMOTEENN(random_state=42)\nX_train_res, y_train_res = smote_enn.fit_resample(X_train, y_train)\n\nprint(\"Before SMOTEENN:\", Counter(y_train))\nprint(\"After SMOTEENN:\", Counter(y_train_res))\n\n# Create LightGBM datasets with free_raw_data=False for incremental training\nlgb_train = lgb.Dataset(X_train_res, label=y_train_res, free_raw_data=False)\nvalid_data = lgb.Dataset(X_test, label=y_test, reference=lgb_train, free_raw_data=False)\n\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'verbosity': -1,\n    'seed': 42\n}\n\nnum_round = 100\nearly_stopping_rounds = 10\n\nbest_score = float('inf')\nbest_iter = 0\nmodel = None\nrounds_without_improve = 0\n\nfor i in range(1, num_round + 1):\n    if model is None:\n        model = lgb.train(params, lgb_train, num_boost_round=1, valid_sets=[valid_data])\n    else:\n        model = lgb.train(params, lgb_train, num_boost_round=1, valid_sets=[valid_data], init_model=model)\n\n    eval_results = model.eval_valid()\n    print(f\"Iteration {i}, eval_valid output: {eval_results}\")\n\n    # Check if eval_results is non-empty and extract logloss\n    if len(eval_results) > 0 and len(eval_results[0]) >= 3:\n        eval_result = eval_results[0][2]\n    else:\n        print(\"Warning: No validation results returned, stopping early.\")\n        break\n\n    print(f\"Iteration {i}, valid logloss: {eval_result:.6f}\")\n\n    if eval_result < best_score:\n        best_score = eval_result\n        best_iter = i\n        rounds_without_improve = 0\n    else:\n        rounds_without_improve += 1\n\n    if rounds_without_improve >= early_stopping_rounds:\n        print(f\"Early stopping at iteration {i}\")\n        break\n\nprint(f\"Best iteration: {best_iter}, Best valid logloss: {best_score:.6f}\")\n\n# Predict on test\ny_proba = model.predict(X_test, num_iteration=best_iter)\ny_pred = (y_proba >= 0.5).astype(int)\n\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, zero_division=0))\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\nroc_auc = roc_auc_score(y_test, y_proba)\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:38:23.349674Z","iopub.execute_input":"2025-09-20T07:38:23.350034Z","iopub.status.idle":"2025-09-20T07:38:24.012757Z","shell.execute_reply.started":"2025-09-20T07:38:23.350012Z","shell.execute_reply":"2025-09-20T07:38:24.010699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\n\n# Ensure evaluation metric is explicitly set\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',  # Explicitly set!\n    'verbosity': -1,\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.05,\n    'num_leaves': 31,\n    'seed': 42\n}\n\n# Build datasets\nlgb_train = lgb.Dataset(X_train_res, label=y_train_res, free_raw_data=False)\nvalid_data = lgb.Dataset(X_test, label=y_test, free_raw_data=False)\n\n# Track eval results\nevals_result = {}\n\n# Train once with early stopping\nmodel = lgb.train(\n    params,\n    lgb_train,\n    num_boost_round=100,\n    valid_sets=[valid_data],\n    valid_names=['valid'],\n    evals_result=evals_result,\n    early_stopping_rounds=10,\n    verbose_eval=True  # set to False if you want to silence output\n)\n\n# Access best iteration\nbest_iter = model.best_iteration\nbest_logloss = evals_result['valid']['binary_logloss'][best_iter - 1]\n\nprint(f\"\\nBest iteration: {best_iter}\")\nprint(f\"Best validation logloss: {best_logloss:.6f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-20T09:43:40.917Z"}},"outputs":[],"execution_count":null}]}